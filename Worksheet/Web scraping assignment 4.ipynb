{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Scrape the details of most viewed videos on YouTube from Wikipedia:<br>\n",
    "Url= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:<br>\n",
    "A) Rank<br>\n",
    "B) Name<br>\n",
    "C) Artist<br>\n",
    "D) Upload date<br>\n",
    "E) View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 30\n",
      "Name 30\n",
      "Artist 30\n",
      "upload 30\n",
      "Views 30\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from this question, we will extract the details of the top watched video on youtube. information is updated under a table \n",
    "in the mentioned link, we will extract all the information one by one from it.\"\"\"\n",
    "\n",
    "# automating the browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "# opening the site\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "# as i have mentioned that we need to extract information from the table thus i am counting the col and row lenght\n",
    "\n",
    "# for header purpose \n",
    "col = driver.find_elements_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/thead/tr/th\")\n",
    "\n",
    "\n",
    "# col and rows columns\n",
    "col = len(driver.find_elements_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/thead/tr/th\")) # we are checing the lenth of columns\n",
    "row = len(driver.find_elements_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr\")) #we are checing the lenth of rows\n",
    "    \n",
    "# created an empty list for saving the mentioned data, running a loop till row count and extracting the information \n",
    "Rank =[]\n",
    "for i in range(1,row+1): # row+1 will give complete count till last row\n",
    "    # have taken the xpath of the first row and col and extracting information one by one by increaning row count(\"+str(r)+\") \n",
    "    rank = driver.find_element_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr[\"+str(i)+\"]/td[1]\")\n",
    "    Rank.append(rank.text)\n",
    "\n",
    "#### extracting all the details in similar fashion as mentioned above in Rank code by increaring the row    \n",
    "Name =[]\n",
    "for i in range(1,row+1):\n",
    "    name = driver.find_element_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr[\"+str(i)+\"]/td[2]\")\n",
    "    Name.append(name.text)\n",
    "    \n",
    "Artist =[]\n",
    "for i in range(1,row+1):\n",
    "    artist = driver.find_element_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr[\"+str(i)+\"]/td[3]\")\n",
    "    Artist.append(artist.text)\n",
    "\n",
    "Upload_date =[]\n",
    "for i in range(1,row+1):\n",
    "    upload = driver.find_element_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr[\"+str(i)+\"]/td[5]\").text\n",
    "    Upload_date.append(upload)\n",
    "    \n",
    "Views =[]\n",
    "for i in range(1,row+1):\n",
    "    view = driver.find_element_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr[\"+str(i)+\"]/td[4]\").text\n",
    "    Views.append(view)\n",
    "    \n",
    "print(\"Rank\",len(Rank))\n",
    "print(\"Name\",len(Name))\n",
    "print(\"Artist\",len(Artist))\n",
    "print(\"upload\",len(Upload_date))\n",
    "print(\"Views\",len(Views))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Scrape the details team Indiaâ€™s international fixtures from bcci.tv.Url = https://www.bcci.tv/. <br>\n",
    "You need to find following details:<br>\n",
    "A) Match title (I.e. 1st ODI)<br>\n",
    "B) Series<br>\n",
    "C) Place<br>\n",
    "D) Date<br>\n",
    "E) Time<br>\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TEST', 'ODI', 'ODI', 'ODI', 'T20I', 'T20I', 'T20I', 'TEST', 'TEST', 'TEST', 'TEST', 'TEST']\n",
      "['ICC WORLD TEST CHAMPIONSHIP', 'SRI LANKA V INDIA 2021', 'SRI LANKA V INDIA 2021', 'SRI LANKA V INDIA 2021', 'SRI LANKA V INDIA 2021', 'SRI LANKA V INDIA 2021', 'SRI LANKA V INDIA 2021', 'ENGLAND V INDIA 2021', 'ENGLAND V INDIA 2021', 'ENGLAND V INDIA 2021', 'ENGLAND V INDIA 2021', 'ENGLAND V INDIA 2021']\n",
      "['The Ageas Bowl, Southampton', 'R Premadasa Stadium, Colombo', 'R Premadasa Stadium, Colombo', 'R Premadasa Stadium, Colombo', 'R Premadasa Stadium, Colombo', 'R Premadasa Stadium, Colombo', 'R Premadasa Stadium, Colombo', 'Trent Bridge, Nottingham', \"Lord's, London\", 'Headingley, Leeds', 'The Oval, London', 'Old Trafford, Manchester']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from this program, we will extract the details about the team india fixture.\"\"\"\n",
    "\n",
    "# automating the browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "# opening the site\n",
    "driver.get(\"https://www.bcci.tv/.\")\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "# fixture = Select(driver.find_element_by_xpath(\"//ul[@class='navigation__list showMoreEnabled']/li[1]/div[1]\"))\n",
    "\n",
    "# fixture.select_by_value(\"Fixtures\")\n",
    "\n",
    "fixture = driver.find_element_by_xpath(\"/html/body/div[3]/div/div[2]/div[2]/nav/ul/li[1]/div[2]/div/ul/li[1]/a\").get_attribute(\"href\")\n",
    "driver.get(fixture)\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "match_title = driver.find_elements_by_xpath(\"//div[@class='fixture__info u-skewed']/div[1]/span[1]\")\n",
    "Match_title = [title.text for title in match_title ]\n",
    "print(Match_title)\n",
    "\n",
    "\n",
    "Series_name = []\n",
    "series_name = driver.find_elements_by_xpath(\"//div[@class='fixture__info u-skewed']/div[1]/span[2]\")\n",
    "for series in series_name:\n",
    "    Series_name.append(series.text)\n",
    "print(Series_name)\n",
    "\n",
    "Place =[]\n",
    "places = driver.find_elements_by_xpath(\"//div[@class='fixture__info u-skewed']/div[2]/p/span\")\n",
    "for place in places:\n",
    "    Place.append(place.text)\n",
    "print(Place)\n",
    "\n",
    "# date_Time = []\n",
    "# tim = driver.find_elements_by_xpath(\"/html/body/div[4]/div/div/div[2]/section/div/div/a/div[2]/div[2]/span\")\n",
    "# for times in tim:\n",
    "#     date_Time.append(times.text.split()[3])\n",
    "# print(Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Friday 18th June 15:00 IST', 'Tuesday 13th July 14:30 IST', 'Friday 16th July 14:30 IST', 'Sunday 18th July 14:30 IST', 'Wednesday 21st July 19:00 IST', 'Friday 23rd July 19:00 IST', 'Sunday 25th July 19:00 IST', 'Wednesday 4th August 15:30 IST', 'Thursday 12th August 15:30 IST', 'Wednesday 25th August 15:30 IST', 'Thursday 2nd September 15:30 IST', 'Friday 10th September 15:30 IST']\n"
     ]
    }
   ],
   "source": [
    "date_Time = []\n",
    "tim = driver.find_elements_by_xpath(\"//div[@class = 'fixture__info u-skewed']/div[2]/span\")\n",
    "for times in tim:\n",
    "    date_Time.append(times.text)\n",
    "print(date_Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Scrape the details of selenium exception from guru99.com.<br>\n",
    "Url = https://www.guru99.com/<br>\n",
    "You need to find following details:<br>\n",
    "A) Name<br>\n",
    "B) Description<br>\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception_name count 42\n",
      "Description count 42\n"
     ]
    }
   ],
   "source": [
    "# we will scrap the details of selenium exception from the page\n",
    "\n",
    "# brower is being automate \n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "# opening site\n",
    "driver.get(\"https://www.guru99.com/\")\n",
    " \n",
    "# maximizing the window\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "# once site is open, we will have to select the selenium option and from there we will have to scrap selenium exception\n",
    "\n",
    "# fecting the selenium page url and saving it sele_exce_url\n",
    "sele_url = driver.find_element_by_xpath(\"//div[@class='row featured-boxes']/div[1]/div/ul[1]/li[3]/a\").get_attribute(\"href\")\n",
    "\n",
    "# opening url\n",
    "\n",
    "driver.get(sele_url)\n",
    "\n",
    "# after going into selenium option, we will have to open the selenium exception handling and scrap the data name and description\n",
    "\n",
    "selenium_exception_url = driver.find_element_by_xpath(\"//div[@class='span12']/div/div[2]/table[@class='table'][5]/tbody/tr[34]/td/a\").get_attribute(\"href\")\n",
    "\n",
    "# opening the selenium exception url \n",
    "\n",
    "driver.get(selenium_exception_url)\n",
    "time.sleep(3)\n",
    "\n",
    "# we have reach to selenium exception handling page from where we can extract the name and description of the exception\n",
    "\n",
    "#let do it by creating a empty list and saving name and description in that one by one by running loop till last row\n",
    "\n",
    "# extrcating lenght of the row\n",
    "row = len(driver.find_elements_by_xpath(\"//*[@id='g-mainbar']/div[1]/div/div/div/div/div/div[2]/table/tbody/tr\"))\n",
    "\n",
    "exception_name = []\n",
    "for i in range(1,row+1):\n",
    "    data = driver.find_element_by_xpath(\"//*[@id='g-mainbar']/div[1]/div/div/div/div/div/div[2]/table/tbody/tr[\"+str(i)+\"]/td[1]\").text\n",
    "    exception_name.append(data)\n",
    "    \n",
    "Description = []\n",
    "for i in range(1,row+1):\n",
    "    desc = driver.find_element_by_xpath(\"//*[@id='g-mainbar']/div[1]/div/div/div/div/div/div[2]/table/tbody/tr[\"+str(i)+\"]/td[2]\").text\n",
    "    Description.append(desc)\n",
    "\n",
    "print(\"exception_name count\",len(exception_name))\n",
    "print(\"Description count\",len(Description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Scrape the details of State-wise GDP of India from statisticstime.com. <br>\n",
    "Url = http://statisticstimes.com/<br>\n",
    "You have to find following details:<br>\n",
    "A) Rank<br>\n",
    "B) State<br>\n",
    "C) GSDP(18-19)<br>\n",
    "D) GSDP(17-18)<br>\n",
    "E) Share(2017)<br>\n",
    "F) GDP($ billion)<br>\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank count = 33\n",
      "GSPD_19_20 count = 33\n",
      "GSPD_18_19 count = 33\n",
      "Share count = 33\n",
      "GDP_in_billion count = 33\n"
     ]
    }
   ],
   "source": [
    "\"\"\"we are going to fetch the GPD data of indian states, we will automate the browser and open the  site and with the help of\n",
    "code we will reach to the destination from where we can fetch the data. \"\"\"\n",
    "\n",
    "# automate the browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#open the website\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "\"\"\"from the headers, we will open the economy option and select the india, as checked all the drop down values have url, \n",
    "we will fetch that url and open to reach to the desire destination\"\"\"\n",
    "\n",
    "\n",
    "emonony_open_india =driver.find_element_by_xpath(\"//*[@id='top']/div[2]/div[2]/div/a[3]\").get_attribute(\"href\") # fetch the url\n",
    "driver.get(emonony_open_india) # open that url\n",
    "time.sleep(3)\n",
    "\n",
    "\"\"\"once we will reach to next page which is from header eomony and selecting india from drop down. \n",
    "we will open GDP of indian state\n",
    "\"\"\"    \n",
    "# we have url to open the indian state GDP hence fetching the same\n",
    "GDP_of_indian_state_page = driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\").get_attribute(\"href\")\n",
    "driver.get(GDP_of_indian_state_page) # opening the url\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# we finally have reached to the page where we will extract the data. we are going to take the count of rows\n",
    "\n",
    "row = len(driver.find_elements_by_xpath(\"//*[@id='table_id']/tbody/tr\")) # counting the rows\n",
    "\n",
    "# extracting the data nows since we have rows count, which will help us to run the loop until\n",
    "\n",
    "Rank =[]\n",
    " # i am extracting the Rank of the state, in this loop i am changing the rows number and table detail is constant\n",
    "for i in range(1,row+1):\n",
    "    Rank.append(driver.find_element_by_xpath(\"//*[@id='table_id']/tbody/tr[\"+str(i)+\"]/td[1]\").text)\n",
    "    \n",
    "State =[]\n",
    " # i am extracting the Rank of the state, in this loop i am changing the rows number and table detail is constant\n",
    "for i in range(1,row+1):\n",
    "    State.append(driver.find_element_by_xpath(\"//*[@id='table_id']/tbody/tr[\"+str(i)+\"]/td[2]\").text)\n",
    "    \n",
    "GSPD_19_20 =[]\n",
    " # i am extracting the Rank of the state, in this loop i am changing the rows number and table detail is constant\n",
    "for i in range(1,row+1):\n",
    "    GSPD_19_20.append(driver.find_element_by_xpath(\"//*[@id='table_id']/tbody/tr[\"+str(i)+\"]/td[3]\").text)\n",
    "    \n",
    "GSPD_18_19 =[]\n",
    " # i am extracting the Rank of the state, in this loop i am changing the rows number and table detail is constant\n",
    "for i in range(1,row+1):\n",
    "    GSPD_18_19.append(driver.find_element_by_xpath(\"//*[@id='table_id']/tbody/tr[\"+str(i)+\"]/td[4]\").text)\n",
    "\n",
    "    \n",
    "Share =[]\n",
    " # i am extracting the Rank of the state, in this loop i am changing the rows number and table detail is constant\n",
    "for i in range(1,row+1):\n",
    "    Share.append(driver.find_element_by_xpath(\"//*[@id='table_id']/tbody/tr[\"+str(i)+\"]/td[5]\").text)\n",
    "    \n",
    "    \n",
    "GDP_in_billion =[]\n",
    " # i am extracting the Rank of the state, in this loop i am changing the rows number and table detail is constant\n",
    "for i in range(1,row+1):\n",
    "    GDP_in_billion.append(driver.find_element_by_xpath(\"//*[@id='table_id']/tbody/tr[\"+str(i)+\"]/td[6]\").text)\n",
    "    \n",
    "print(\"Rank count =\",len(Rank))    \n",
    "print(\"GSPD_19_20 count =\",len(GSPD_19_20))\n",
    "print(\"GSPD_18_19 count =\",len(GSPD_18_19))\n",
    "print(\"Share count =\",len(Share))\n",
    "print(\"GDP_in_billion count =\",len(GDP_in_billion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 Scrape the details of top 100 songs on billiboard.com.<br>\n",
    "Url = https://www.billboard.com/<br>\n",
    "You have to find the following details:<br>\n",
    "A) Song name<br>\n",
    "B) Artist name<br>\n",
    "C) Last week rank<br>\n",
    "D) Peak rank<br>\n",
    "E) Weeks on board<br>\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We will scrap certian details from the billborad.com site, everything will be automated by code\"\"\"\n",
    "\n",
    "# Browser is being automate and then we will open the site by taking the name\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "# opening a site\n",
    "driver.get(\"https://www.billboard.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# once site is open we need to go to the hot 100 page and extract the song, thus that is being automate by fecting its url\n",
    "\n",
    "hot_100 = driver.find_element_by_xpath(\"//*[@id='root']/div[2]/div[2]/nav/ul/li[3]/a\").get_attribute(\"href\")\n",
    "driver.get(hot_100)\n",
    "time.sleep(3)\n",
    "\n",
    "# we have reached to the page from where we will have to extract the information and we are going to do it one by one\n",
    "\n",
    "# i am going to extract song name\n",
    "\n",
    "Song_name = []\n",
    "songs = driver.find_elements_by_xpath(\"//div[@class='chart-list__wrapper']/div/ol/li/button/span[2]/span[1]\")\n",
    "for i in songs:\n",
    "    Song_name.append(i.text)\n",
    "\n",
    "# we are extracting the author name\n",
    "\n",
    "Artist_name = []\n",
    "Artist = driver.find_elements_by_xpath(\"//div[@class='chart-list__wrapper']/div/ol/li/button/span[2]/span[2]\")\n",
    "for i in Artist:\n",
    "    Artist_name.append(i.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 Scrape the details of Data science recruiters from naukri.com.<br>\n",
    "Url = https://www.naukri.com/<br>\n",
    "You have to find the following details:<br>\n",
    "A) Name<br>\n",
    "B) Designation <br>\n",
    "C) Company<br>\n",
    "D) Skills they hire for<br>\n",
    "E) Location<br>\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and \n",
    "click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We will scrap certian details from the naukri.com site, everything will be automated by code\"\"\"\n",
    "\n",
    "# Browser is being automate and then we will open the site by taking the name\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "# opening a site\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "# till now website has been open, we will have to go to recruiter option from here and then search for data science job and scrap\n",
    "\n",
    "recruiter_option = driver.find_element_by_xpath(\"//*[@id='root']/div[1]/div/ul[1]/li[2]/div/ul/li[1]/a\").get_attribute(\"href\")\n",
    "driver.get(recruiter_option)\n",
    "\n",
    "# we have reach to the recruiter page, now search for data science job, automating search bar\n",
    "\n",
    "search_bar = driver.find_element_by_xpath(\"//*[@id='skill']/div[1]/div[2]/input\") # it will allow us to control search bar\n",
    "search_bar.send_keys(\"Data science\") #sending input to search bar\n",
    "\n",
    "# click on submit button\n",
    "\n",
    "driver.find_element_by_id(\"qsbFormBtn\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "# i am going to extract the desire information. we will extract mentioned details one by one\n",
    "name = driver.find_elements_by_xpath(\"//*[@id='tabP-1']/div/div/div/div/p/a/span\")\n",
    "Name =[names.text for names in name]\n",
    "\n",
    "# Extracting designation from the sites\n",
    "Desig = driver.find_elements_by_xpath(\"//*[@id='tabP-1']/div/div/div/div/p/span[1]\")\n",
    "Designation =[post.text for post in Desig]\n",
    "\n",
    "\n",
    "# extracting company name\n",
    "company = driver.find_elements_by_xpath(\"//*[@id='tabP-1']/div/div/div/div/p/a[2]\")\n",
    "Company =[com.text for com in company]\n",
    "\n",
    "\n",
    "# extracting locatoin\n",
    "Location = []\n",
    "try:\n",
    "    loc = driver.find_elements_by_xpath(\"//*[@id='tabP-1']/div/div/div/div/p/span[2]\")\n",
    "    for i in loc:\n",
    "        Location.append(i.text)\n",
    "        time.sleep(1)\n",
    "except:\n",
    "    Location.append(\"-\")\n",
    "\n",
    "# extracting Skills\n",
    "skill = driver.find_elements_by_xpath(\"//*[@id='tabP-1']/div/div/div/div[2]\")\n",
    "SKill =[skills.text for skills in skill]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "50\n",
      "48\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(Name))\n",
    "print(len(Designation))\n",
    "print(len(Company))\n",
    "print(len(Location))\n",
    "print(len(SKill))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 Scrape the details of Highest selling novels.<br>\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/<br>\n",
    "You have to find the following details:<br>\n",
    "A) Book name<br>\n",
    "B) Author name<br>\n",
    "C) Volumes sold<br>\n",
    "D) Publisher<br>\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 100\n",
      "Author 100\n",
      "Volumne 100\n",
      "Publisher_name 100\n",
      "Genre_name 100\n"
     ]
    }
   ],
   "source": [
    "\"\"\" We are going to scrap books details from mentioned site, everything will be automated by code\"\"\"\n",
    "\n",
    "# Browser is being automate and then we will open the site by taking the name\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "# opening a site\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "#we have reached to the page from where we will extract the required details\n",
    "# the first details that we are going to extract is book name followed by author name volumnes sold, publisher and Genre\n",
    "\n",
    "# Extracting book name\n",
    "Book = driver.find_elements_by_xpath(\"//table[@class='in-article sortable']/tbody/tr/td[2]\")\n",
    "Book_name =[book.text for book in Book]\n",
    "\n",
    "# extracting Author name\n",
    "Author = driver.find_elements_by_xpath(\"//table[@class='in-article sortable']/tbody/tr/td[3]\")\n",
    "Author_name =[author.text for author in Author]\n",
    "\n",
    "#extracting Volume_sold number\n",
    "Volumes = driver.find_elements_by_xpath(\"//table[@class='in-article sortable']/tbody/tr/td[4]\")\n",
    "Volumes_sold =[v_sold.text for v_sold in Volumes]\n",
    "\n",
    "# publisher name\n",
    "Publisher = driver.find_elements_by_xpath(\"//table[@class='in-article sortable']/tbody/tr/td[5]\")\n",
    "Publisher_name =[publis.text for publis in Publisher]\n",
    "\n",
    "# genre name\n",
    "Genre = driver.find_elements_by_xpath(\"//table[@class='in-article sortable']/tbody/tr/td[6]\")\n",
    "Genre_name =[genre.text for genre in Genre]\n",
    "\n",
    "\n",
    "print(\"Book\",len(Book_name))\n",
    "print(\"Author\",len(Author_name))\n",
    "print(\"Volumne\",len(Volumes_sold))\n",
    "print(\"Publisher_name\",len(Publisher_name))\n",
    "print(\"Genre_name\",len(Genre_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9 Scrape the details most watched tv series of all time from imdb.com.<br>\n",
    " Url = https://www.imdb.com/list/ls095964455/<br>\n",
    "You have to find the following details:<br>\n",
    "A) Name<br>\n",
    "B) Year span<br>\n",
    "C) Genre<br>\n",
    "D) Run time<br>\n",
    "E) Ratings<br>\n",
    "F) Votes<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series_name 100\n",
      "year_span 100\n",
      "Genre_name 100\n",
      "Run_time 100\n",
      "Rating 100\n",
      "Vote_count 100\n"
     ]
    }
   ],
   "source": [
    "\"\"\" We are going to scrap TV series details from mentioned site, everything will be automated by code\"\"\"\n",
    "\n",
    "# Browser is being automate and then we will open the site by taking the name\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "# opening a site\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# site has been opened, from here we will extract the details of the most watched TV series\n",
    "\n",
    "# we are extracting TV series name\n",
    "name = driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/h3/a\")\n",
    "Series_name = [names.text for names in name]\n",
    "\n",
    "# extracting year of span\n",
    "Years = driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/h3/span[2]\")\n",
    "year_span = [year.text for year in Years]\n",
    "\n",
    "\n",
    "#extracting Genre name\n",
    "genre = driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/p/span[5]\")\n",
    "Genre_name = [Genre.text for Genre in genre]\n",
    "\n",
    "# Extracting run time details\n",
    "run = driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/p/span[3]\")\n",
    "Run_time = [Run.text for Run in run]\n",
    "\n",
    "# Extracting Ranking\n",
    "rank = driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/div/div/span[2]\")\n",
    "Rating = [Rank.text for Rank in rank]\n",
    "\n",
    "\n",
    "# Extractng vote count\n",
    "Vote = driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/p[4]/span[2]\")\n",
    "Vote_count = [vote.text for vote in Vote]\n",
    "\n",
    "print(\"Series_name\",len(Series_name))\n",
    "print(\"year_span\",len(year_span))\n",
    "print(\"Genre_name\",len(Genre_name))\n",
    "print(\"Run_time\",len(Run_time))\n",
    "print(\"Rating\",len(Rating))\n",
    "print(\"Vote_count\",len(Vote_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10  Details of Datasets from UCI machine learning repositories. <br>\n",
    "Url = https://archive.ics.uci.edu/<br>\n",
    "You have to find the following details:<br>\n",
    "A) Dataset name<br>\n",
    "B) Data type<br>\n",
    "C) Task<br>\n",
    "D) Attribute type<br>\n",
    "E) No of instances<br>\n",
    "F) No of attribute<br>\n",
    "G) Year<br>\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name 589\n",
      "datatype_name 589\n",
      "Task_name 589\n",
      "Attributes_type 589\n",
      "No_of_instances 589\n",
      "No_of_attributes 589\n",
      "Years 589\n"
     ]
    }
   ],
   "source": [
    "\"\"\" We are going to scrap datasets details from mentioned site, everything will be automated by code\"\"\"\n",
    "\n",
    "# Browser is being automate and then we will open the site by taking the name\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "# opening a site\n",
    "driver.get(\"https://archive.ics.uci.edu/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# site has been opened, now we are going to click on view all dataset and wil reach to the all dataset\n",
    "\n",
    "all_dataset = driver.find_element_by_xpath(\"/html/body/table[2]/tbody/tr/td/span/b/a\").get_attribute(\"href\")\n",
    "driver.get(all_dataset)\n",
    "time.sleep(5)\n",
    "\n",
    "# we have reached to the page and extracting the data\n",
    "#Name of the dataset\n",
    "dataset = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[1]\")\n",
    "dataset_name =[data.text for data in dataset]\n",
    "\n",
    "# Extracting data type\n",
    "\n",
    "data_type = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]\")\n",
    "datatype_name =[dtype.text for dtype in data_type]\n",
    "\n",
    "# Extracting the task\n",
    "task_type = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]\")\n",
    "Task_name =[task.text for task in task_type]\n",
    "\n",
    "# extracting attributes types\n",
    "attribute = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]\")\n",
    "Attributes_type =[attr.text for attr in attribute]\n",
    "\n",
    "# extracting attributes types\n",
    "instances = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]\")\n",
    "No_of_instances =[instan.text for instan in instances]\n",
    "\n",
    "# extracting attributes types\n",
    "No_attribute = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[6]\")\n",
    "No_of_attributes =[no_attr.text for no_attr in No_attribute]\n",
    "\n",
    "\n",
    "# extracting attributes types\n",
    "year = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]\")\n",
    "Years =[Year.text for Year in year]\n",
    "\n",
    "print(\"dataset name\",len(dataset_name))\n",
    "print(\"datatype_name\",len(datatype_name))\n",
    "print(\"Task_name\",len(Task_name))\n",
    "print(\"Attributes_type\",len(Attributes_type))\n",
    "print(\"No_of_instances\",len(No_of_instances))\n",
    "print(\"No_of_attributes\",len(No_of_attributes))\n",
    "print(\"Years\",len(Years))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
